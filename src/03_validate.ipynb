{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 03 Data Validation Pipeline\n",
        "\n",
        "This notebook implements a comprehensive data quality framework that:\n",
        "1. Validates bronze data against silver tables\n",
        "2. Creates silver tables if they don't exist\n",
        "3. Efficiently tracks changes using row hashes\n",
        "4. Provides JSON audit reports\n",
        "5. Implements data quality checks (types, ranges, NA handling)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Create the validate module if it doesn't exist\n",
        "from petrinex.validate import (\n",
        "    DataValidator,\n",
        "    create_silver_from_bronze,\n",
        "    compare_bronze_silver,\n",
        "    generate_audit_report\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define data paths\n",
        "bronze_path = Path(\"../fixtures\")\n",
        "silver_path = Path(\"../fixtures\")  # Silver tables in fixtures\n",
        "audit_path = Path(\"../fixtures\")   # Audit files in fixtures\n",
        "\n",
        "# Create fixtures directory if it doesn't exist\n",
        "bronze_path.mkdir(exist_ok=True)\n",
        "\n",
        "# Define datasets to validate\n",
        "datasets = {\n",
        "    'ngl_vol': 'ngl_vol_bronze_cvx.parquet',\n",
        "    'conv_vol': 'conv_vol_bronze_cvx.parquet'\n",
        "}\n",
        "\n",
        "print(\"üìÅ Validation Configuration:\")\n",
        "print(f\"   Bronze path: {bronze_path}\")\n",
        "print(f\"   Silver path: {silver_path}\")\n",
        "print(f\"   Audit path: {audit_path}\")\n",
        "print(f\"   Datasets: {list(datasets.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define data quality rules for Petrinex bronze tables\n",
        "data_quality_rules = {\n",
        "    'ngl_vol': {\n",
        "        'required_columns': [\n",
        "            'ReportingFacilityID', 'ProductionMonth', 'WellID',\n",
        "            'GasProduction', 'OilProduction', 'CondensateProduction', \n",
        "            'WaterProduction', 'Hours'\n",
        "        ],\n",
        "        'data_types': {\n",
        "            'ReportingFacilityID': 'string',\n",
        "            'ProductionMonth': 'datetime',\n",
        "            'WellID': 'string', \n",
        "            'GasProduction': 'float',\n",
        "            'OilProduction': 'float',\n",
        "            'CondensateProduction': 'float',\n",
        "            'WaterProduction': 'float',\n",
        "            'Hours': 'float',\n",
        "            'OperatorBAID': 'string',\n",
        "            'OperatorName': 'string'\n",
        "        },\n",
        "        'ranges': {\n",
        "            'GasProduction': (0, 1000000),\n",
        "            'OilProduction': (0, 100000),\n",
        "            'CondensateProduction': (0, 50000), \n",
        "            'WaterProduction': (0, 500000),\n",
        "            'Hours': (0, 744)  # Max hours in a month\n",
        "        },\n",
        "        'rate_columns': ['GasProduction', 'OilProduction', 'CondensateProduction', 'WaterProduction'],\n",
        "        'cumulative_columns': []\n",
        "    },\n",
        "    'conv_vol': {\n",
        "        'required_columns': [\n",
        "            'ProductionMonth', 'OperatorBAID', 'ReportingFacilityID', \n",
        "            'Volume', 'Hours'\n",
        "        ],\n",
        "        'data_types': {\n",
        "            'ProductionMonth': 'datetime',\n",
        "            'OperatorBAID': 'string',\n",
        "            'OperatorName': 'string',\n",
        "            'ReportingFacilityID': 'string',\n",
        "            'Volume': 'float',\n",
        "            'Energy': 'float',\n",
        "            'Hours': 'float'\n",
        "        },\n",
        "        'ranges': {\n",
        "            'Volume': (0, 1000000),\n",
        "            'Energy': (0, 10000000),\n",
        "            'Hours': (0, 744)\n",
        "        },\n",
        "        'rate_columns': ['Volume'],\n",
        "        'cumulative_columns': []\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üìã Data Quality Rules Configured:\")\n",
        "for dataset, rules in data_quality_rules.items():\n",
        "    print(f\"   {dataset}: {len(rules['required_columns'])} columns, {len(rules['ranges'])} range checks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the data validator\n",
        "validator = DataValidator()\n",
        "\n",
        "print(\"‚úÖ Data Validator initialized\")\n",
        "print(f\"   Available validation methods: {len(validator.get_validation_methods())}\")\n",
        "\n",
        "# Process datasets\n",
        "validation_results = {}\n",
        "\n",
        "for dataset_name, filename in datasets.items():\n",
        "    print(f\"\\nüîç Processing {dataset_name.upper()}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Load bronze data\n",
        "    bronze_file = bronze_path / filename\n",
        "    if not bronze_file.exists():\n",
        "        print(f\"‚ùå Bronze file not found: {bronze_file}\")\n",
        "        continue\n",
        "        \n",
        "    bronze_df = pd.read_parquet(bronze_file)\n",
        "    print(f\"üìä Loaded bronze data: {len(bronze_df):,} rows, {len(bronze_df.columns)} columns\")\n",
        "    \n",
        "    # Define silver file path\n",
        "    silver_file = silver_path / f\"{dataset_name}_silver.parquet\"\n",
        "    \n",
        "    # Check if silver table exists\n",
        "    if not silver_file.exists():\n",
        "        print(\"üÜï No silver table found - creating initial silver table\")\n",
        "        \n",
        "        # Validate and clean bronze data\n",
        "        rules = data_quality_rules.get(dataset_name, {})\n",
        "        validation_result = validator.validate_dataframe(bronze_df, rules)\n",
        "        cleaned_df = validator.apply_data_quality_fixes(bronze_df, rules)\n",
        "        \n",
        "        # Create silver table\n",
        "        silver_df = create_silver_from_bronze(cleaned_df)\n",
        "        silver_df.to_parquet(silver_file, index=False)\n",
        "        \n",
        "        validation_results[dataset_name] = {\n",
        "            'action': 'created_silver',\n",
        "            'bronze_rows': len(bronze_df),\n",
        "            'silver_rows': len(silver_df),\n",
        "            'validation': validation_result,\n",
        "            'changes': None\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ Created silver table: {len(silver_df):,} rows\")\n",
        "        \n",
        "    else:\n",
        "        print(\"üîÑ Silver table exists - performing change detection\")\n",
        "        \n",
        "        # Load existing silver data\n",
        "        silver_df = pd.read_parquet(silver_file)\n",
        "        print(f\"üìä Loaded silver data: {len(silver_df):,} rows\")\n",
        "        \n",
        "        # Validate bronze data\n",
        "        rules = data_quality_rules.get(dataset_name, {})\n",
        "        validation_result = validator.validate_dataframe(bronze_df, rules)\n",
        "        cleaned_bronze_df = validator.apply_data_quality_fixes(bronze_df, rules)\n",
        "        \n",
        "        # Compare bronze vs silver\n",
        "        comparison_result = compare_bronze_silver(cleaned_bronze_df, silver_df)\n",
        "        \n",
        "        # Update silver table if there are changes\n",
        "        if comparison_result['has_changes']:\n",
        "            print(f\"üìù Changes detected: {comparison_result['summary']}\")\n",
        "            \n",
        "            # Create updated silver table\n",
        "            updated_silver_df = create_silver_from_bronze(cleaned_bronze_df)\n",
        "            updated_silver_df.to_parquet(silver_file, index=False)\n",
        "            \n",
        "            print(f\"‚úÖ Updated silver table: {len(updated_silver_df):,} rows\")\n",
        "        else:\n",
        "            print(\"‚úÖ No changes detected - silver table up to date\")\n",
        "        \n",
        "        validation_results[dataset_name] = {\n",
        "            'action': 'compared_and_updated' if comparison_result['has_changes'] else 'no_changes',\n",
        "            'bronze_rows': len(bronze_df),\n",
        "            'silver_rows': len(silver_df),\n",
        "            'validation': validation_result,\n",
        "            'changes': comparison_result\n",
        "        }\n",
        "    \n",
        "    # Display validation summary\n",
        "    result = validation_results[dataset_name]\n",
        "    validation = result['validation']\n",
        "    \n",
        "    print(f\"\\nüìã Data Quality Summary for {dataset_name}:\")\n",
        "    print(f\"   ‚úÖ Passed checks: {validation['passed_checks']}\")\n",
        "    print(f\"   ‚ùå Failed checks: {validation['failed_checks']}\")\n",
        "    if validation['errors']:\n",
        "        print(f\"   üö® Errors: {len(validation['errors'])}\")\n",
        "        for error in validation['errors'][:3]:  # Show first 3 errors\n",
        "            print(f\"      - {error}\")\n",
        "\n",
        "print(f\"\\nüéâ Validation completed for {len(validation_results)} datasets\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive audit report\n",
        "audit_report = generate_audit_report(validation_results)\n",
        "\n",
        "# Save audit report\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "audit_file = audit_path / f\"validation_audit_{timestamp}.json\"\n",
        "\n",
        "with open(audit_file, 'w') as f:\n",
        "    json.dump(audit_report, f, indent=2, default=str)\n",
        "\n",
        "print(f\"üìÑ Audit report saved: {audit_file}\")\n",
        "print(f\"üìä Report summary:\")\n",
        "print(f\"   - Timestamp: {audit_report['metadata']['timestamp']}\")\n",
        "print(f\"   - Datasets processed: {len(audit_report['datasets'])}\")\n",
        "print(f\"   - Total validation checks: {audit_report['summary']['total_checks']}\")\n",
        "print(f\"   - Passed checks: {audit_report['summary']['passed_checks']}\")\n",
        "print(f\"   - Failed checks: {audit_report['summary']['failed_checks']}\")\n",
        "\n",
        "# Display comprehensive validation dashboard\n",
        "print(\"\\nüìä VALIDATION DASHBOARD\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for dataset_name, result in validation_results.items():\n",
        "    print(f\"\\nüóÇÔ∏è  {dataset_name.upper()}\")\n",
        "    print(f\"   Action: {result['action']}\")\n",
        "    print(f\"   Bronze rows: {result['bronze_rows']:,}\")\n",
        "    print(f\"   Silver rows: {result['silver_rows']:,}\")\n",
        "    \n",
        "    validation = result['validation']\n",
        "    total_checks = validation['passed_checks'] + validation['failed_checks']\n",
        "    success_rate = validation['passed_checks'] / total_checks * 100 if total_checks > 0 else 0\n",
        "    \n",
        "    print(f\"   Data Quality: {success_rate:.1f}% ({validation['passed_checks']}/{total_checks} checks passed)\")\n",
        "    \n",
        "    if result['changes'] and result['changes']['has_changes']:\n",
        "        changes = result['changes']\n",
        "        print(f\"   Changes detected:\")\n",
        "        print(f\"     - New rows: {changes.get('new_rows', 0)}\")\n",
        "        print(f\"     - Modified rows: {changes.get('modified_rows', 0)}\")\n",
        "        print(f\"     - Deleted rows: {changes.get('deleted_rows', 0)}\")\n",
        "\n",
        "print(f\"\\n‚úÖ All datasets validated successfully!\")\n",
        "print(f\"üìÅ Silver tables and audit reports available in: {silver_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
