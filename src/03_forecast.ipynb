{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee353e42-ff58-4955-9608-12865bd0950e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Forecasting\n",
    "\n",
    "In order to generate models, quality checks, reserves, etc., we need forecasts of the well production. This notebook demonstrates:\n",
    "\n",
    "1. **Data Processing**: Preparing well production data for forecasting\n",
    "2. **ARPS Decline Curves**: Automatically fitting exponential, hyperbolic, and harmonic decline curves\n",
    "3. **Forecasting**: Generating production forecasts for individual wells\n",
    "\n",
    "## ARPS Decline Curves\n",
    "\n",
    "The Arps decline curve equations are fundamental tools in petroleum engineering for forecasting oil and gas production:\n",
    "\n",
    "- **Exponential Decline (b=0)**: `q(t) = qi * exp(-Di * t)`\n",
    "- **Hyperbolic Decline (0<b<1)**: `q(t) = qi * (1 + b * Di * t)^(-1/b)`\n",
    "- **Harmonic Decline (b=1)**: `q(t) = qi / (1 + Di * t)`\n",
    "\n",
    "Where:\n",
    "- `qi` = initial production rate\n",
    "- `Di` = initial decline rate\n",
    "- `b` = decline exponent\n",
    "- `t` = time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bca260b-13d1-448f-8082-30b60a85c9ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from petrinex.config import DotConfig\n",
    "from petrinex.forecast import (\n",
    "    forecast_spark_workflow\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97deaa26-71e3-4c93-94c9-200b68d76175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = DotConfig(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PetrinexForecasting\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ada180c-329e-4fa8-ba20-0e3e82918273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the input table (replace with your actual silver table)\n",
    "input_table = f\"{config.catalog}.{config.schema}.ngl_silver\"\n",
    "\n",
    "# Check if the table exists and get basic info\n",
    "try:\n",
    "    input_df = spark.table(input_table)\n",
    "    row_count = input_df.count()\n",
    "    well_count = input_df.select(\"WellID\").distinct().count()\n",
    "    print(f\"Input table: {input_table}\")\n",
    "    print(f\"Total rows: {row_count:,}\")\n",
    "    print(f\"Unique wells: {well_count:,}\")\n",
    "    \n",
    "    # Show sample of the data\n",
    "    print(\"\\nSample data:\")\n",
    "    input_df.show(5)\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing table {input_table}: {e}\")\n",
    "    print(\"Please ensure the silver table exists or use alternative data source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b87ba93-b635-4387-8294-b165c14ec255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run Spark forecast workflow with memory-efficient batching\n",
    "output_tables = forecast_spark_workflow(\n",
    "    spark=spark,\n",
    "    config=config,\n",
    "    input_table=input_table,\n",
    "    # Optional overrides:\n",
    "    # batch_size=300,  # Smaller batches for very large datasets\n",
    "    # forecast_months=24,  # Override config default\n",
    "    # curve_type='auto',\n",
    "    # min_r_squared=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c324f0cc-a1aa-4b10-92f0-3a406d795fa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The workflow automatically uses batching for memory efficiency\n",
    "# Adjust batch_size parameter above if you need smaller batches for very large datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79d7b22f-f7e1-43cb-ab36-98d88459e199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify and explore the output tables\n",
    "if 'output_tables' in locals():\n",
    "    for production_type, tables in output_tables.items():\n",
    "        print(f\"=== {production_type} Tables ===\")\n",
    "        \n",
    "        for table_type, table_name in tables.items():\n",
    "            table_df = spark.table(table_name)\n",
    "            row_count = table_df.count()\n",
    "            print(f\"{table_type.title()}: {table_name} ({row_count:,} rows)\")\n",
    "            \n",
    "            if row_count > 0:\n",
    "                table_df.show(3)\n",
    "else:\n",
    "    print(\"No output tables available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e38b604f-d45b-4b33-aac9-369e80bb76bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced forecast analysis with ARPS parameters\n",
    "if 'output_tables' in locals() and 'GasProduction' in output_tables:\n",
    "    # Get summary table for gas production\n",
    "    summary_table_name = output_tables['GasProduction']['summary']\n",
    "    summary_df = spark.table(summary_table_name)\n",
    "    \n",
    "    # Convert to Pandas for analysis and plotting\n",
    "    summary_pd = summary_df.toPandas()\n",
    "    \n",
    "    print(f\"Gas Production Forecast Summary:\")\n",
    "    print(f\"Wells forecast: {len(summary_pd)}\")\n",
    "    print(f\"Forecast date: {summary_pd['ForecastDate'].iloc[0]}\")\n",
    "    print(f\"Average R-squared: {summary_pd['RSquared'].mean():.3f}\")\n",
    "    print(f\"Average AIC: {summary_pd['AIC'].mean():.1f}\")\n",
    "    print(f\"Curve types: {summary_pd['CurveType'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Display ARPS parameter ranges\n",
    "    print(f\"\\nARPS Parameter Ranges:\")\n",
    "    print(f\"Initial Rate (qi): {summary_pd['InitialRate_qi'].min():.1f} - {summary_pd['InitialRate_qi'].max():.1f}\")\n",
    "    print(f\"Decline Rate (di): {summary_pd['DeclineRate_di'].min():.3f} - {summary_pd['DeclineRate_di'].max():.3f}\")\n",
    "    \n",
    "    # Show data coverage\n",
    "    print(f\"\\nData Coverage:\")\n",
    "    print(f\"Earliest data: {summary_pd['HistoricalDataMinDate'].min()}\")\n",
    "    print(f\"Latest data: {summary_pd['HistoricalDataMaxDate'].max()}\")\n",
    "    print(f\"Average data points per well: {summary_pd['DataPointsUsed'].mean():.1f}\")\n",
    "    \n",
    "    # Enhanced plotting\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # R-squared histogram\n",
    "    ax1.hist(summary_pd['RSquared'], bins=20, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_xlabel('R-squared')\n",
    "    ax1.set_ylabel('Number of Wells')\n",
    "    ax1.set_title('Forecast Quality Distribution')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Curve type pie chart\n",
    "    curve_counts = summary_pd['CurveType'].value_counts()\n",
    "    ax2.pie(curve_counts.values, labels=curve_counts.index, autopct='%1.1f%%')\n",
    "    ax2.set_title('Decline Curve Types')\n",
    "    \n",
    "    # Initial rate vs decline rate scatter\n",
    "    ax3.scatter(summary_pd['InitialRate_qi'], summary_pd['DeclineRate_di'], \n",
    "                c=summary_pd['RSquared'], cmap='viridis', alpha=0.6)\n",
    "    ax3.set_xlabel('Initial Rate (qi)')\n",
    "    ax3.set_ylabel('Decline Rate (di)')\n",
    "    ax3.set_title('ARPS Parameters (colored by RÂ²)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Data points vs R-squared\n",
    "    ax4.scatter(summary_pd['DataPointsUsed'], summary_pd['RSquared'], alpha=0.6)\n",
    "    ax4.set_xlabel('Data Points Used')\n",
    "    ax4.set_ylabel('R-squared')\n",
    "    ax4.set_title('Data Quality vs Forecast Quality')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display enhanced summary table sample\n",
    "    print(f\"\\nSample Enhanced Summary (first 3 wells):\")\n",
    "    display_cols = ['WellID', 'CurveType', 'RSquared', 'InitialRate_qi', 'DeclineRate_di', \n",
    "                   'DataPointsUsed', 'HistoricalDataMinDate', 'HistoricalDataMaxDate']\n",
    "    summary_pd[display_cols].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8075efbe-2570-444e-9b23-1914257626b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tables are stored in Spark/Delta format for efficient querying\n",
    "# Use spark.table('table_name') to access the full datasets\n",
    "\n",
    "# Optional: Export samples for external analysis\n",
    "# if 'output_tables' in locals():\n",
    "#     for production_type, tables in output_tables.items():\n",
    "#         summary_df = spark.table(tables['summary'])\n",
    "#         summary_sample = summary_df.limit(1000).toPandas()\n",
    "#         summary_sample.to_parquet(f\"../fixtures/{production_type.lower()}_forecast_summary.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e49cf11-d1cd-4043-953a-a2df221131d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Summary\n",
    "if 'output_tables' in locals():\n",
    "    total_production_types = len(output_tables)\n",
    "    total_tables = sum(len(tables) for tables in output_tables.values())\n",
    "    \n",
    "    print(\"=== FORECASTING COMPLETE ===\")\n",
    "    print(f\"Production types: {total_production_types}\")\n",
    "    print(f\"Tables created: {total_tables}\")\n",
    "    print(f\"Config: {config.catalog}.{config.schema}, {config.forecast.horizon_months}mo horizon, {config.forecast.min_months}mo minimum data\")\n",
    "    \n",
    "    print(f\"\\nOutput tables:\")\n",
    "    for production_type, tables in output_tables.items():\n",
    "        print(f\"\\n{production_type}:\")\n",
    "        for table_type, table_name in tables.items():\n",
    "            count = spark.table(table_name).count()\n",
    "            print(f\"  {table_type}: {table_name} ({count:,} rows)\")\n",
    "    \n",
    "    print(f\"\\nNext steps:\")\n",
    "    print(f\"- Analyze forecast quality with summary tables\")\n",
    "    print(f\"- Use forecast tables for production planning\") \n",
    "    print(f\"- Use combined tables for visualization\")\n",
    "    \n",
    "else:\n",
    "    print(\"No forecasting workflow completed.\")\n",
    "    print(\"Ensure input table exists and wells have sufficient historical data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d37c6e0a-6a53-4bdd-afd2-ce7d0d9be3ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_forecast",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
