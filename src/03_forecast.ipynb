{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee353e42-ff58-4955-9608-12865bd0950e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Forecasting\n",
    "\n",
    "In order to generate models, quality checks, reserves, etc., we need forecasts of the well production. This notebook demonstrates:\n",
    "\n",
    "1. **Data Processing**: Preparing well production data for forecasting\n",
    "2. **ARPS Decline Curves**: Automatically fitting exponential, hyperbolic, and harmonic decline curves\n",
    "3. **Forecasting**: Generating production forecasts for individual wells\n",
    "\n",
    "## ARPS Decline Curves\n",
    "\n",
    "The Arps decline curve equations are fundamental tools in petroleum engineering for forecasting oil and gas production:\n",
    "\n",
    "- **Exponential Decline (b=0)**: `q(t) = qi * exp(-Di * t)`\n",
    "- **Hyperbolic Decline (0<b<1)**: `q(t) = qi * (1 + b * Di * t)^(-1/b)`\n",
    "- **Harmonic Decline (b=1)**: `q(t) = qi / (1 + Di * t)`\n",
    "\n",
    "Where:\n",
    "- `qi` = initial production rate\n",
    "- `Di` = initial decline rate\n",
    "- `b` = decline exponent\n",
    "- `t` = time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bca260b-13d1-448f-8082-30b60a85c9ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from databricks.connect import DatabricksSession as SparkSession\n",
    "\n",
    "from petrinex.config import DotConfig\n",
    "from petrinex.forecast import forecast_spark_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97deaa26-71e3-4c93-94c9-200b68d76175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession as SparkSession\n",
    "\n",
    "if \"spark\" not in locals():\n",
    "    spark = SparkSession.builder.serverless(True).getOrCreate()\n",
    "\n",
    "config = DotConfig(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build calendar and normalized tables (idempotent)\n",
    "from petrinex.process import build_ngl_calendar_and_normalized_tables\n",
    "\n",
    "spark = SparkSession.builder.serverless(True).getOrCreate()\n",
    "build_ngl_calendar_and_normalized_tables(spark, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.serverless(True).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b87ba93-b635-4387-8294-b165c14ec255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:petrinex.forecast:Insufficient data points for curve fitting: 0\n",
      "WARNING:petrinex.forecast:Well ABWI100011906218W500: Poor fit (R² = 0.000)\n",
      "WARNING:petrinex.forecast:Insufficient data points for curve fitting: 0\n",
      "WARNING:petrinex.forecast:Well ABWI100041406219W502: Poor fit (R² = 0.000)\n",
      "WARNING:petrinex.forecast:Insufficient data points for curve fitting: 0\n",
      "WARNING:petrinex.forecast:Well ABWI103072206218W502: Poor fit (R² = 0.000)\n",
      "WARNING:petrinex.forecast:Insufficient data points for curve fitting: 0\n",
      "WARNING:petrinex.forecast:Well ABWI100012206216W500: Poor fit (R² = 0.000)\n",
      "WARNING:petrinex.forecast:Insufficient data points for curve fitting: 0\n",
      "WARNING:petrinex.forecast:Well ABWI100022906223W500: Poor fit (R² = 0.000)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'summary_table_name' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run Spark forecast workflow with memory-efficient batching\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m output_tables = \u001b[43mforecast_spark_workflow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspark\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_table\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mshm.petrinex.ngl_silver\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/Petrinex/src/petrinex/forecast.py:977\u001b[39m, in \u001b[36mforecast_spark_workflow\u001b[39m\u001b[34m(spark, config, input_table, **kwargs)\u001b[39m\n\u001b[32m    973\u001b[39m         combined_table_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.catalog\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.schema\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproduction_type.lower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_historical_forecast_combined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    974\u001b[39m         _write_pandas_to_spark_table(spark, final_combined, combined_table_name)\n\u001b[32m    976\u001b[39m     output_tables[production_type] = {\n\u001b[32m--> \u001b[39m\u001b[32m977\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43msummary_table_name\u001b[49m,\n\u001b[32m    978\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mforecast\u001b[39m\u001b[33m\"\u001b[39m: forecast_table_name,\n\u001b[32m    979\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcombined\u001b[39m\u001b[33m\"\u001b[39m: combined_table_name,\n\u001b[32m    980\u001b[39m     }\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output_tables\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'summary_table_name' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "output_tables = forecast_spark_workflow(\n",
    "    spark=spark,\n",
    "    config=config,\n",
    "    use_normalized=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c324f0cc-a1aa-4b10-92f0-3a406d795fa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The workflow automatically uses batching for memory efficiency\n",
    "# Adjust batch_size parameter above if you need smaller batches for very large datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79d7b22f-f7e1-43cb-ab36-98d88459e199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify and explore the output tables\n",
    "if 'output_tables' in locals():\n",
    "    for production_type, tables in output_tables.items():\n",
    "        print(f\"=== {production_type} Tables ===\")\n",
    "        \n",
    "        for table_type, table_name in tables.items():\n",
    "            table_df = spark.table(table_name)\n",
    "            row_count = table_df.count()\n",
    "            print(f\"{table_type.title()}: {table_name} ({row_count:,} rows)\")\n",
    "            \n",
    "            if row_count > 0:\n",
    "                table_df.show(3)\n",
    "else:\n",
    "    print(\"No output tables available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e38b604f-d45b-4b33-aac9-369e80bb76bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced forecast analysis with ARPS parameters\n",
    "if 'output_tables' in locals() and 'GasProduction' in output_tables:\n",
    "    # Get summary table for gas production\n",
    "    summary_table_name = output_tables['GasProduction']['summary']\n",
    "    summary_df = spark.table(summary_table_name)\n",
    "    \n",
    "    # Convert to Pandas for analysis and plotting\n",
    "    summary_pd = summary_df.toPandas()\n",
    "    \n",
    "    print(f\"Gas Production Forecast Summary:\")\n",
    "    print(f\"Wells forecast: {len(summary_pd)}\")\n",
    "    print(f\"Forecast date: {summary_pd['ForecastDate'].iloc[0]}\")\n",
    "    print(f\"Average R-squared: {summary_pd['RSquared'].mean():.3f}\")\n",
    "    print(f\"Average AIC: {summary_pd['AIC'].mean():.1f}\")\n",
    "    print(f\"Curve types: {summary_pd['CurveType'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Display ARPS parameter ranges\n",
    "    print(f\"\\nARPS Parameter Ranges:\")\n",
    "    print(f\"Initial Rate (qi): {summary_pd['InitialRate_qi'].min():.1f} - {summary_pd['InitialRate_qi'].max():.1f}\")\n",
    "    print(f\"Decline Rate (di): {summary_pd['DeclineRate_di'].min():.3f} - {summary_pd['DeclineRate_di'].max():.3f}\")\n",
    "    \n",
    "    # Show data coverage\n",
    "    print(f\"\\nData Coverage:\")\n",
    "    print(f\"Earliest data: {summary_pd['HistoricalDataMinDate'].min()}\")\n",
    "    print(f\"Latest data: {summary_pd['HistoricalDataMaxDate'].max()}\")\n",
    "    print(f\"Average data points per well: {summary_pd['DataPointsUsed'].mean():.1f}\")\n",
    "    \n",
    "    # Enhanced plotting\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # R-squared histogram\n",
    "    ax1.hist(summary_pd['RSquared'], bins=20, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_xlabel('R-squared')\n",
    "    ax1.set_ylabel('Number of Wells')\n",
    "    ax1.set_title('Forecast Quality Distribution')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Curve type pie chart\n",
    "    curve_counts = summary_pd['CurveType'].value_counts()\n",
    "    ax2.pie(curve_counts.values, labels=curve_counts.index, autopct='%1.1f%%')\n",
    "    ax2.set_title('Decline Curve Types')\n",
    "    \n",
    "    # Initial rate vs decline rate scatter\n",
    "    ax3.scatter(summary_pd['InitialRate_qi'], summary_pd['DeclineRate_di'], \n",
    "                c=summary_pd['RSquared'], cmap='viridis', alpha=0.6)\n",
    "    ax3.set_xlabel('Initial Rate (qi)')\n",
    "    ax3.set_ylabel('Decline Rate (di)')\n",
    "    ax3.set_title('ARPS Parameters (colored by R²)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Data points vs R-squared\n",
    "    ax4.scatter(summary_pd['DataPointsUsed'], summary_pd['RSquared'], alpha=0.6)\n",
    "    ax4.set_xlabel('Data Points Used')\n",
    "    ax4.set_ylabel('R-squared')\n",
    "    ax4.set_title('Data Quality vs Forecast Quality')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display enhanced summary table sample\n",
    "    print(f\"\\nSample Enhanced Summary (first 3 wells):\")\n",
    "    display_cols = ['WellID', 'CurveType', 'RSquared', 'InitialRate_qi', 'DeclineRate_di', \n",
    "                   'DataPointsUsed', 'HistoricalDataMinDate', 'HistoricalDataMaxDate']\n",
    "    summary_pd[display_cols].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Volumes/shm/petrinex/bronze/conventional/*.CSV'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_path_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Volumes/shm/petrinex/bronze/conventional/*.CSV'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_path_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Volumes/shm/petrinex/bronze/conventional/*.CSV'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_path_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Volumes/shm/petrinex/bronze/conventional/*.CSV'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_path_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8075efbe-2570-444e-9b23-1914257626b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tables are stored in Spark/Delta format for efficient querying\n",
    "# Use spark.table('table_name') to access the full datasets\n",
    "\n",
    "# Optional: Export samples for external analysis\n",
    "# if 'output_tables' in locals():\n",
    "#     for production_type, tables in output_tables.items():\n",
    "#         summary_df = spark.table(tables['summary'])\n",
    "#         summary_sample = summary_df.limit(1000).toPandas()\n",
    "#         summary_sample.to_parquet(f\"../fixtures/{production_type.lower()}_forecast_summary.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e49cf11-d1cd-4043-953a-a2df221131d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Summary\n",
    "if 'output_tables' in locals():\n",
    "    total_production_types = len(output_tables)\n",
    "    total_tables = sum(len(tables) for tables in output_tables.values())\n",
    "    \n",
    "    print(\"=== FORECASTING COMPLETE ===\")\n",
    "    print(f\"Production types: {total_production_types}\")\n",
    "    print(f\"Tables created: {total_tables}\")\n",
    "    print(f\"Config: {config.catalog}.{config.schema}, {config.forecast.horizon_months}mo horizon, {config.forecast.min_months}mo minimum data\")\n",
    "    \n",
    "    print(f\"\\nOutput tables:\")\n",
    "    for production_type, tables in output_tables.items():\n",
    "        print(f\"\\n{production_type}:\")\n",
    "        for table_type, table_name in tables.items():\n",
    "            count = spark.table(table_name).count()\n",
    "            print(f\"  {table_type}: {table_name} ({count:,} rows)\")\n",
    "    \n",
    "    print(f\"\\nNext steps:\")\n",
    "    print(f\"- Analyze forecast quality with summary tables\")\n",
    "    print(f\"- Use forecast tables for production planning\") \n",
    "    print(f\"- Use combined tables for visualization\")\n",
    "    \n",
    "else:\n",
    "    print(\"No forecasting workflow completed.\")\n",
    "    print(\"Ensure input table exists and wells have sufficient historical data.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_forecast",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
